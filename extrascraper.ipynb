{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bn6_Brr3UK9z",
        "outputId": "2338b169-06d9-4cf8-ec62-649ae77f9894"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikidata.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract relevant content, e.g., paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = {\n",
        "    \"Pittsburgh Symphony\": \"https://en.wikipedia.org/wiki/Pittsburgh_Symphony_Orchestra\",\n",
        "    \"Pittsburgh Opera\": \"https://en.wikipedia.org/wiki/Pittsburgh_Opera\",\n",
        "    \"Pittsburgh Cultural Trust\": \"https://en.wikipedia.org/wiki/Pittsburgh_Cultural_Trust\",\n",
        "    \"Carnegie Museums\": \"https://en.wikipedia.org/wiki/Carnegie_Museums_of_Pittsburgh\",\n",
        "    \"Heinz History Center\": \"https://en.wikipedia.org/wiki/Heinz_History_Center\",\n",
        "    \"The Frick Pittsburgh\": \"https://en.wikipedia.org/wiki/Frick_Art_%26_Historical_Center\",\n",
        "    \"List of Museums in Pittsburgh\": \"https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\",\n",
        "    \"Picklesburgh\": \"https://en.wikipedia.org/wiki/Picklesburgh\"\n",
        "}\n",
        "\n",
        "# Scrape and save content to CSV\n",
        "output_file = 'wikidata.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Page Name\", \"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_name, page_url in pages.items():\n",
        "        content = scrape_page(page_url)\n",
        "        if content:\n",
        "            writer.writerow([page_name, page_url, content])\n",
        "\n",
        "output_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract relevant content, e.g., paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = {\n",
        "    \"Pittsburgh Symphony\": \"https://en.wikipedia.org/wiki/Pittsburgh_Symphony_Orchestra\",\n",
        "    \"Pittsburgh Opera\": \"https://en.wikipedia.org/wiki/Pittsburgh_Opera\",\n",
        "    \"Pittsburgh Cultural Trust\": \"https://en.wikipedia.org/wiki/Pittsburgh_Cultural_Trust\",\n",
        "    \"Carnegie Museums\": \"https://en.wikipedia.org/wiki/Carnegie_Museums_of_Pittsburgh\",\n",
        "    \"Heinz History Center\": \"https://en.wikipedia.org/wiki/Heinz_History_Center\",\n",
        "    \"The Frick Pittsburgh\": \"https://en.wikipedia.org/wiki/Frick_Art_%26_Historical_Center\",\n",
        "    \"List of Museums in Pittsburgh\": \"https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\",\n",
        "    \"Picklesburgh\": \"https://en.wikipedia.org/wiki/Picklesburgh\"\n",
        "}\n",
        "\n",
        "# Scrape and save content to CSV\n",
        "output_file = 'wikidata.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Page Name\", \"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_name, page_url in pages.items():\n",
        "        content = scrape_page(page_url)\n",
        "        if content:\n",
        "            writer.writerow([page_name, page_url, content])\n",
        "\n",
        "output_file\n"
      ],
      "metadata": {
        "id": "RGgKtSkIURV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape the webpage content and return the text content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extracting paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return \"\"\n",
        "\n",
        "# List of URLs to scrape\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/List_of_baseball_parks_in_Pittsburgh\",\n",
        "    \"https://carnegieart.org/art/whats-on-view/\",\n",
        "    \"https://carnegiemnh.org/explore/explore/exhibitions/\",\n",
        "    \"https://carnegiesciencecenter.org/exhibits/\",\n",
        "    \"https://www.warhol.org/exhibitions/\",\n",
        "    \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\",\n",
        "    \"https://en.wikipedia.org/wiki/Politics_of_Pennsylvania\",\n",
        "    \"https://en.wikipedia.org/wiki/Category:Sports_venues_in_Pittsburgh\"\n",
        "]\n",
        "\n",
        "# CSV file to store the scraped data\n",
        "csv_filename = \"scraped_data.csv\"\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"URL\", \"Content\"])\n",
        "\n",
        "    # Loop through the URLs and scrape content\n",
        "    for url in urls:\n",
        "        print(f\"Scraping {url} ...\")\n",
        "        content = scrape_page(url)\n",
        "\n",
        "        # Write the content along with the URL to the CSV file\n",
        "        writer.writerow([url, content])\n",
        "\n",
        "print(f\"Scraping completed. Data saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hccyB36atKBS",
        "outputId": "56118e1e-289a-494c-a98b-4e1f4668ebd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping https://en.wikipedia.org/wiki/List_of_baseball_parks_in_Pittsburgh ...\n",
            "Scraping https://carnegieart.org/art/whats-on-view/ ...\n",
            "Scraping https://carnegiemnh.org/explore/explore/exhibitions/ ...\n",
            "Scraping https://carnegiesciencecenter.org/exhibits/ ...\n",
            "Scraping https://www.warhol.org/exhibitions/ ...\n",
            "Scraping https://en.wikipedia.org/wiki/History_of_Pittsburgh ...\n",
            "Scraping https://en.wikipedia.org/wiki/Politics_of_Pennsylvania ...\n",
            "Scraping https://en.wikipedia.org/wiki/Category:Sports_venues_in_Pittsburgh ...\n",
            "Scraping completed. Data saved to scraped_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape a webpage\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to scrape {url}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape exhibition pages\n",
        "def scrape_exhibition_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        exhibitions = soup.find_all('div', class_='exhibition')  # Adjust class name based on page structure\n",
        "        content = \"\\n\".join([ex.get_text(strip=True) for ex in exhibitions if ex.get_text(strip=True)])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to scrape {url}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = [\n",
        "    {\"name\": \"Baseball Parks in Pittsburgh\", \"url\": \"https://en.wikipedia.org/wiki/List_of_baseball_parks_in_Pittsburgh\"},\n",
        "    {\"name\": \"Carnegie Art - What's on View\", \"url\": \"https://carnegieart.org/art/whats-on-view/\"},\n",
        "    {\"name\": \"Carnegie MNH Exhibitions\", \"url\": \"https://carnegiemnh.org/explore/explore/exhibitions/\"},\n",
        "    {\"name\": \"Carnegie Science Center Exhibits\", \"url\": \"https://carnegiesciencecenter.org/exhibits/\"},\n",
        "    {\"name\": \"Warhol Museum Exhibitions\", \"url\": \"https://www.warhol.org/exhibitions/\"},\n",
        "    {\"name\": \"History of Pittsburgh\", \"url\": \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\"},\n",
        "    {\"name\": \"Politics of Pennsylvania\", \"url\": \"https://en.wikipedia.org/wiki/Politics_of_Pennsylvania\"},\n",
        "    {\"name\": \"Sports Venues in Pittsburgh\", \"url\": \"https://en.wikipedia.org/wiki/Category:Sports_venues_in_Pittsburgh\"},\n",
        "    {\"name\": \"Pennsylvania Congressional Districts\", \"url\": \"https://en.wikipedia.org/wiki/Pennsylvania%27s_congressional_districts\"},\n",
        "    {\"name\": \"Mayors of Pittsburgh\", \"url\": \"https://en.wikipedia.org/wiki/List_of_mayors_of_Pittsburgh\"}\n",
        "]\n",
        "\n",
        "# Prepare CSV file to save the scraped data\n",
        "output_file = 'scraped_data.csv'\n",
        "\n",
        "# Open CSV file and write headers\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Page Name', 'URL', 'Scraped Content']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Scrape each page and write data to CSV\n",
        "    for page in pages:\n",
        "        print(f\"Scraping {page['name']} ...\")\n",
        "        if \"exhibition\" in page['url']:  # Assuming exhibition pages need a different scraper\n",
        "            content = scrape_exhibition_page(page['url'])\n",
        "        else:\n",
        "            content = scrape_page(page['url'])\n",
        "        if content:\n",
        "            writer.writerow({'Page Name': page['name'], 'URL': page['url'], 'Scraped Content': content})\n",
        "        else:\n",
        "            print(f\"Skipping {page['name']} due to scraping issue.\")\n",
        "\n",
        "print(f\"Data scraped and saved to {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFZ128EJuB6Q",
        "outputId": "ccde8bac-1e2a-4534-e79b-8213e25180a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Baseball Parks in Pittsburgh ...\n",
            "Scraping Carnegie Art - What's on View ...\n",
            "Scraping Carnegie MNH Exhibitions ...\n",
            "Skipping Carnegie MNH Exhibitions due to scraping issue.\n",
            "Scraping Carnegie Science Center Exhibits ...\n",
            "Scraping Warhol Museum Exhibitions ...\n",
            "Skipping Warhol Museum Exhibitions due to scraping issue.\n",
            "Scraping History of Pittsburgh ...\n",
            "Scraping Politics of Pennsylvania ...\n",
            "Scraping Sports Venues in Pittsburgh ...\n",
            "Scraping Pennsylvania Congressional Districts ...\n",
            "Scraping Mayors of Pittsburgh ...\n",
            "Data scraped and saved to scraped_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape the webpage\n",
        "def scrape_neighborhoods(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        neighborhoods = soup.find_all('div', class_='views-row')\n",
        "\n",
        "        # Extract the neighborhood names and URLs\n",
        "        neighborhood_list = []\n",
        "        for neighborhood in neighborhoods:\n",
        "            name = neighborhood.find('div', class_='field--name-name').get_text(strip=True)\n",
        "            link = neighborhood.find('a')['href']\n",
        "            full_link = \"https://secretpittsburgh.pitt.edu\" + link\n",
        "            neighborhood_list.append([name, full_link])\n",
        "\n",
        "        return neighborhood_list\n",
        "    else:\n",
        "        print(f\"Failed to scrape {url}\")\n",
        "        return None\n",
        "\n",
        "# URL of the webpage to scrape\n",
        "url = \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/4\"\n",
        "\n",
        "# Scrape the webpage\n",
        "neighborhoods = scrape_neighborhoods(url)\n",
        "\n",
        "# Save the scraped data to a CSV file\n",
        "output_file = \"neighborhoods.csv\"\n",
        "if neighborhoods:\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"Neighborhood\", \"URL\"])  # Write the header\n",
        "        writer.writerows(neighborhoods)  # Write the data\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No data to save.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNXnSozTsxk-",
        "outputId": "e9c1034b-95b3-42d5-d4a0-edc31fa2219b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to neighborhoods.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape a single neighborhood page\n",
        "def scrape_neighborhood_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract all paragraphs on the page\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to scrape {url}\")\n",
        "        return None\n",
        "\n",
        "# Neighborhood data with links\n",
        "neighborhoods = [\n",
        "    {\"name\": \"Oakland\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/4\"},\n",
        "    {\"name\": \"North Side\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/6\"},\n",
        "    {\"name\": \"Bloomfield\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/7\"},\n",
        "    {\"name\": \"Downtown\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/8\"},\n",
        "    {\"name\": \"Homestead\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/9\"},\n",
        "    {\"name\": \"Homewood\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/10\"},\n",
        "    {\"name\": \"Millvale\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/11\"},\n",
        "    {\"name\": \"Point Breeze\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/12\"},\n",
        "    {\"name\": \"Shadyside\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/13\"},\n",
        "    {\"name\": \"Beechview\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/14\"},\n",
        "    {\"name\": \"Squirrel Hill\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/15\"},\n",
        "    {\"name\": \"Swissvale\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/21\"},\n",
        "    {\"name\": \"East Liberty\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/22\"},\n",
        "    {\"name\": \"South Side\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/23\"},\n",
        "    {\"name\": \"Hill District\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/24\"},\n",
        "    {\"name\": \"Mt. Washington\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/25\"},\n",
        "    {\"name\": \"Braddock\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/26\"},\n",
        "    {\"name\": \"Brookline\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/27\"},\n",
        "    {\"name\": \"Lawrenceville\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/28\"},\n",
        "    {\"name\": \"Garfield\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/29\"},\n",
        "    {\"name\": \"Highland Park\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/30\"},\n",
        "    {\"name\": \"Washington's Landing\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/31\"},\n",
        "    {\"name\": \"Oakmont\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/32\"},\n",
        "    {\"name\": \"Duquesne\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/33\"},\n",
        "    {\"name\": \"Strip District\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/35\"},\n",
        "    {\"name\": \"West Mifflin\", \"url\": \"https://secretpittsburgh.pitt.edu/sp/taxonomy/term/36\"}\n",
        "]\n",
        "\n",
        "# File to save the scraped content\n",
        "output_file = \"neighborhood_data.csv\"\n",
        "\n",
        "# Scrape each neighborhood and save content to CSV\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Neighborhood\", \"URL\", \"Content\"])  # Write the header\n",
        "\n",
        "    for neighborhood in neighborhoods:\n",
        "        name = neighborhood[\"name\"]\n",
        "        url = neighborhood[\"url\"]\n",
        "        content = scrape_neighborhood_page(url)\n",
        "        if content:\n",
        "            writer.writerow([name, url, content])\n",
        "        else:\n",
        "            writer.writerow([name, url, \"Failed to retrieve content\"])\n",
        "\n",
        "print(f\"Scraped data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qUcpb-jwIQG",
        "outputId": "681ca7a3-b559-46ab-c6c7-86e484a2a2a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to neighborhood_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract relevant content, e.g., paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = {\n",
        "    \"Oct\": \"https://pittsburgh.events/october/\",\n",
        "    \"Nov\": \"https://pittsburgh.events/november/\",\n",
        "    \"Dec\": \"https://pittsburgh.events/december/\",\n",
        "    \"Jan\": \"https://pittsburgh.events/january/\",\n",
        "    \"Feb\": \"https://pittsburgh.events/february\",\n",
        "    \"Mar\": \"https://pittsburgh.events/march\",\n",
        "    \"Apr\": \"https://pittsburgh.events/april\",\n",
        "    \"May\": \"https://pittsburgh.events/may\",\n",
        "    \"Jun\": \"https://pittsburgh.events/june\",\n",
        "    \"Jul\": \"https://pittsburgh.events/july\",\n",
        "    \"Aug\": \"https://pittsburgh.events/august\",\n",
        "    \"Sep\": \"https://pittsburgh.events/september\"\n",
        "}\n",
        "\n",
        "# Scrape and save content to CSV\n",
        "output_file = 'events_data.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Page Name\", \"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_name, page_url in pages.items():\n",
        "        content = scrape_page(page_url)\n",
        "        if content:\n",
        "            writer.writerow([page_name, page_url, content])\n",
        "\n",
        "output_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YoTLyCcR0lAL",
        "outputId": "8a9ce9c5-4a3e-44e2-e55d-69dd68f9cc1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'events_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract relevant content, e.g., paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = {\n",
        "    \"Downtown_Pittsburgh\": \"https://downtownpittsburgh.com/events/\",\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# Scrape and save content to CSV\n",
        "output_file = 'dt_pitt_data.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Page Name\", \"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_name, page_url in pages.items():\n",
        "        content = scrape_page(page_url)\n",
        "        if content:\n",
        "            writer.writerow([page_name, page_url, content])\n",
        "\n",
        "output_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zR1TnZ46GcoJ",
        "outputId": "4ebe86c6-7084-48b5-930f-9623e9045f51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dt_pitt_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all event blocks\n",
        "        events = []\n",
        "        for event_block in soup.find_all('div', class_='eventitem'):\n",
        "            # Extract event category\n",
        "            category = event_block.find('div', class_='term').get_text(strip=True) if event_block.find('div', class_='term') else \"No category\"\n",
        "\n",
        "            # Extract event title\n",
        "            title_tag = event_block.find('h1').find('a')\n",
        "            title = title_tag.get_text(strip=True) if title_tag else \"No title\"\n",
        "\n",
        "            # Extract event date and time\n",
        "            date_time = event_block.find('div', class_='eventdate').get_text(strip=True) if event_block.find('div', class_='eventdate') else \"No date\"\n",
        "\n",
        "            # Extract event description if available\n",
        "            description = event_block.find('div', class_='copyContent').get_text(strip=True) if event_block.find('div', class_='copyContent') else \"No description\"\n",
        "\n",
        "            # Extract event detail link\n",
        "            detail_link = title_tag['href'] if title_tag else \"No link\"\n",
        "            full_detail_link = f\"https://downtownpittsburgh.com{detail_link}\"\n",
        "\n",
        "            # Format the scraped content for the event\n",
        "            event_content = f\"Title: {title}\\nCategory: {category}\\nDate & Time: {date_time}\\nDescription: {description}\\nDetail Link: {full_detail_link}\"\n",
        "\n",
        "            # Append the scraped URL and content\n",
        "            events.append([full_detail_link, event_content])\n",
        "\n",
        "        return events\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# URL to scrape events\n",
        "url = 'https://downtownpittsburgh.com/events/'\n",
        "\n",
        "# Scrape the page and save content to CSV\n",
        "output_file = 'downtown_pittsburgh_events.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"URL\", \"Scraped Content\"])\n",
        "\n",
        "    # Get the events from the page\n",
        "    events = scrape_page(url)\n",
        "    if events:\n",
        "        writer.writerows(events)\n",
        "\n",
        "print(f\"Scraped data has been saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZLdDcjFJ7i1",
        "outputId": "4151d81b-3d19-4b95-c94c-bb648fa4b6bc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data has been saved to downtown_pittsburgh_events.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        events = []\n",
        "        # Find all event blocks\n",
        "        for event_block in soup.find_all('div', class_='fdn-pres-content'):\n",
        "            # Extract event title\n",
        "            title_tag = event_block.find('p', class_='fdn-teaser-headline').find('a')\n",
        "            title = title_tag.get_text(strip=True) if title_tag else \"No title\"\n",
        "\n",
        "            # Extract event date and time\n",
        "            date_time = event_block.find('p', class_='fdn-teaser-subheadline').get_text(strip=True) if event_block.find('p', class_='fdn-teaser-subheadline') else \"No date\"\n",
        "\n",
        "            # Extract event location\n",
        "            location_tag = event_block.find('p', class_='fdn-event-teaser-location')\n",
        "            location = location_tag.get_text(strip=True) if location_tag else \"No location\"\n",
        "\n",
        "            # Extract event description\n",
        "            description_tag = event_block.find('div', class_='fdn-teaser-description')\n",
        "            description = description_tag.get_text(strip=True) if description_tag else \"No description\"\n",
        "\n",
        "            # Extract event detail link\n",
        "            detail_link = title_tag['href'] if title_tag else \"No link\"\n",
        "            full_detail_link = f\"https://www.pghcitypaper.com{detail_link}\"\n",
        "\n",
        "            # Format the scraped content for the event\n",
        "            event_content = f\"Title: {title}\\nDate & Time: {date_time}\\nLocation: {location}\\nDescription: {description}\\nDetail Link: {full_detail_link}\"\n",
        "\n",
        "            # Append the scraped URL and content\n",
        "            events.append([full_detail_link, event_content])\n",
        "\n",
        "        return events\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to find the total number of pages\n",
        "def find_total_pages(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find the pagination block\n",
        "        pagination = soup.find('ul', class_='uk-pagination')\n",
        "        if pagination:\n",
        "            pages = pagination.find_all('a', class_='fdn-page-navigation-page')\n",
        "            # The last page number is usually the highest number\n",
        "            return int(pages[-1].get_text(strip=True))\n",
        "        else:\n",
        "            return 1\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url} for pagination. Status code: {response.status_code}\")\n",
        "        return 1\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://www.pghcitypaper.com/pittsburgh/EventSearch'\n",
        "\n",
        "# Find the total number of pages\n",
        "total_pages = find_total_pages(base_url)\n",
        "\n",
        "# Scrape all pages\n",
        "output_file = 'pgh_city_paper_all_events.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_num in range(1, total_pages + 1):\n",
        "        page_url = f\"{base_url}?page={page_num}&sortType=date&v=d\"\n",
        "        print(f\"Scraping page {page_num} of {total_pages}: {page_url}\")\n",
        "        events = scrape_page(page_url)\n",
        "        if events:\n",
        "            writer.writerows(events)\n",
        "\n",
        "print(f\"Scraped data has been saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--TnTf-kNu9H",
        "outputId": "a7c5637c-96df-422f-a589-9f04469ccfe0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=1&sortType=date&v=d\n",
            "Scraping page 2 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=2&sortType=date&v=d\n",
            "Scraping page 3 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=3&sortType=date&v=d\n",
            "Scraping page 4 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=4&sortType=date&v=d\n",
            "Scraping page 5 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=5&sortType=date&v=d\n",
            "Scraping page 6 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&sortType=date&v=d\n",
            "Scraping page 7 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&sortType=date&v=d\n",
            "Scraping page 8 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&sortType=date&v=d\n",
            "Scraping page 9 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&sortType=date&v=d\n",
            "Scraping page 10 of 10: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&sortType=date&v=d\n",
            "Scraped data has been saved to pgh_city_paper_all_events.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract relevant content, e.g., paragraphs\n",
        "        paragraphs = soup.find_all('ul')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = {\n",
        "    \"cmu events\": \"https://events.cmu.edu/all\",\n",
        "\n",
        "}\n",
        "\n",
        "# Scrape and save content to CSV\n",
        "output_file = 'cmu_events_data.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Page Name\", \"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_name, page_url in pages.items():\n",
        "        content = scrape_page(page_url)\n",
        "        if content:\n",
        "            writer.writerow([page_name, page_url, content])\n",
        "\n",
        "output_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "krAkWNMhOOfl",
        "outputId": "660cb9f7-4528-4027-87c3-3ea11e21f24a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cmu_events_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and return page content\n",
        "def scrape_page(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract relevant content, e.g., paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# List of pages to scrape\n",
        "pages = {\n",
        "    \"Calendar\": \"https://www.pittsburghsymphony.org/calendar\",\n",
        "    \"About\": \"https://www.pittsburghsymphony.org/pso_home/web/about-landing\",\n",
        "    \"Musicians\": \"https://www.pittsburghsymphony.org/pso_home/web/musicians\",\n",
        "    \"Music Director\": \"https://www.pittsburghsymphony.org/pso_home/biographies/pso-conductors/honeck-manfred\",\n",
        "    \"Careers\": \"https://www.pittsburghsymphony.org/pso_home/web/about-landing/career-opportunities\",\n",
        "    \"President & CEO\": \"https://www.pittsburghsymphony.org/pso_home/web/about-landing/melia-p-tourangeau-president\",\n",
        "    \"Tickets\": \"https://www.pittsburghsymphony.org/pso_home/web/tickets-landing\",\n",
        "    \"Subscriptions\": \"https://www.pittsburghsymphony.org/pso_home/web/subscriptions/why-subscribe-24-25\",\n",
        "    \"Learning Programs\": \"https://www.pittsburghsymphony.org/pso_home/web/community-landing/learning-programs\",\n",
        "    \"Seating Charts\": \"https://www.pittsburghsymphony.org/pso_home/web/tickets-landing/seating-charts\",\n",
        "    \"Donate Now\": \"https://give.pittsburghsymphony.org/donate-now\",\n",
        "    \"Corporate Partnerships\": \"https://www.pittsburghsymphony.org/pso_home/web/give-landing/corporate-partnerships\",\n",
        "    \"Gala\": \"https://one.bidpal.net/psogala2024/welcome\",\n",
        "    \"Shop\": \"http://pittsburghsymphonyorchestra.bigcartel.com/\",\n",
        "}\n",
        "\n",
        "# Scrape and save content to CSV\n",
        "output_file = 'symphony_data.csv'\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Page Name\", \"URL\", \"Scraped Content\"])\n",
        "\n",
        "    for page_name, page_url in pages.items():\n",
        "        content = scrape_page(page_url)\n",
        "        if content:\n",
        "            writer.writerow([page_name, page_url, content])\n",
        "\n",
        "output_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DFOoHrN3Pgcu",
        "outputId": "b63e9271-2909-4041-c244-946b201f0f0c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'symphony_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySKKOcUHR7w6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}