{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD1yLKdKMeFE"
      },
      "source": [
        "# Retreival System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxP8xdhRMeFF"
      },
      "source": [
        "This is based heavily on the tutorial from https://huggingface.co/learn/cookbook/en/advanced_rag as recommended in the course. We should probably credit it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters to test:\n",
        "\n",
        "\n",
        "* embedding models\n",
        "* distance strategies for vector store\n",
        "* chunk size\n",
        "* overlap size\n",
        "* k value for top_k\n",
        "* model used for LLM\n",
        "* alter generated prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "iJukJBF5MIdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community transformers sentence-transformers faiss-gpu bitsandbytes"
      ],
      "metadata": {
        "id": "jmG3wBh7bE98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "aniJEiDdb7ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used for embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "print(f\"Model's maximum sequence length: {SentenceTransformer('all-mpnet-base-v2').max_seq_length}\")"
      ],
      "metadata": {
        "id": "CBsNAPtKiiZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Initialize embeddings\n",
        "# embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "# Split text into chunks\n",
        "TEXT_SEPARATORS = [\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \".\"\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=384-64, # selected to stay under 384 max size for all-mpnet-base-v2\n",
        "    chunk_overlap=50, # arbitrarily pick how much across chunks\n",
        "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "    separators=TEXT_SEPARATORS,\n",
        ")\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    try:\n",
        "      chunks = text_splitter.split_text(row['text'])\n",
        "      texts.extend(chunks)\n",
        "      metadatas.extend([{'source': row['source']}] * len(chunks))\n",
        "    except:\n",
        "      print(f\"source {row['source']} corrupted\")"
      ],
      "metadata": {
        "id": "oZlCrH_Sb8mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write a list of strings to a text file\n",
        "def write_list_to_file(string_list, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for string in string_list:\n",
        "            file.write(string + '\\n')\n",
        "\n",
        "# Example usage\n",
        "write_list_to_file(texts, 'output.txt')"
      ],
      "metadata": {
        "id": "uDLGDyktjh2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(text) for text in texts]\n",
        "\n",
        "# Plot the distribution of text lengths, counted as the number of tokens\n",
        "import matplotlib.pyplot as plt\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLYIznRkeDQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS index\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "vectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadatas, distance_strategy=DistanceStrategy.COSINE)"
      ],
      "metadata": {
        "id": "rAD62n3cb-6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define retrieval function\n",
        "def retrieve_top_k(query, k=5):\n",
        "    results = vectorstore.similarity_search(query, k=k)\n",
        "    return [(res.page_content, res.metadata['source']) for res in results]"
      ],
      "metadata": {
        "id": "Ny1a2lZJcDIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage of retrieval only\n",
        "# query = \"When is CMU's Spring Carnival Weekend 2025?\"\n",
        "# top_chunks = retrieve_top_k(query, k=5)\n",
        "# for i, (text, source) in enumerate(top_chunks, 1):\n",
        "#     print(f\"\\nResult {i}:\")\n",
        "#     print(f\"Source: {source}\")\n",
        "#     print(f\"Text: {text}\\n\")"
      ],
      "metadata": {
        "id": "lIjYNyOCa_g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reader"
      ],
      "metadata": {
        "id": "A4de5-jvO0dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_template():\n",
        "  prompt_in_chat_format = [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"\"\"Using the information contained in the context,\n",
        "        give a concise answer to the question.\n",
        "        if possible limit your answer to single or a few words for who, when, where questions.\n",
        "        wherever possible extract name, date, or title without additional explanations.\n",
        "        Respond only to the question asked, response should be concise and relevant to the question.\n",
        "        You should do short answers format responses. DO NOT PUT \"ANSWER\" BEFORE THE ANSWER.\n",
        "        Don't answer in full sentences. For example, say \"12\" instead of \"The answer is 12.\"\n",
        "        \"\"\",\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"\"\"Context:\n",
        "          {context}\n",
        "          ---\n",
        "          Here are some examples of question answer pairs:\n",
        "\n",
        "          Who is Pittsburgh named after?\n",
        "          William Pitt\n",
        "\n",
        "          What famous machine learning venue had its first conference in Pittsburgh in 1980?\n",
        "          ICML\n",
        "\n",
        "          What musical artist is performing at PPG Arena on October 13?\n",
        "          Billie Eilish\n",
        "\n",
        "          ---\n",
        "          Don't answer in full sentences. For example, say \"12\" instead of \"The answer is 12.\"\n",
        "          Now here is the question for you to answer:\n",
        "\n",
        "          {question}\n",
        "          \"\"\",\n",
        "      },\n",
        "  ]\n",
        "  RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "      prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "  )\n",
        "  return RAG_PROMPT_TEMPLATE\n",
        "\n",
        "def create_prompt(template, query, top_chunks):\n",
        "  context = \"\\n\\n\".join([chunk[0] for chunk in top_chunks])\n",
        "  return template.format(question=query, context=context)"
      ],
      "metadata": {
        "id": "lygyo49_O1Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "pTZ9J3K8aNxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "READER_MODEL_NAME = \"stabilityai/stablelm-zephyr-3b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "bfpSbBDQQ46S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prompt template\n",
        "template = create_template()\n",
        "\n",
        "# Initialize LLM Pipeline\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=300,\n",
        ")"
      ],
      "metadata": {
        "id": "rlVNWiLSWJR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" in Week 3, and where was the game played?\"\n",
        "top_chunks = retrieve_top_k(query, k=5)\n",
        "prompt = create_prompt(template, query, top_chunks)\n",
        "READER_LLM(prompt)"
      ],
      "metadata": {
        "id": "KFcCtWWWWqK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def get_answer(question):\n",
        "    # Simulating a RAG system query\n",
        "    top_chunks = retrieve_top_k(question, k=3)\n",
        "    prompt = create_prompt(template, question, top_chunks)\n",
        "    return READER_LLM(prompt)[0]['generated_text']\n",
        "\n",
        "\n",
        "def generate_answers(input_file, output_file):\n",
        "\n",
        "    with open(input_file, 'r') as file:\n",
        "        questions = file.readlines()\n",
        "\n",
        "    answers = []\n",
        "    print(\"Number of Q's: \" + str(len(questions)))\n",
        "    for index, question in tqdm(enumerate(questions)):\n",
        "        question = question.strip()\n",
        "        if question:\n",
        "            answer = get_answer(question)\n",
        "            answers.append(f'Question {index + 1}: {answer}')\n",
        "\n",
        "    with open(output_file, 'w') as file:\n",
        "        for answer in answers:\n",
        "            file.write(answer + '\\n')\n",
        "            print(answer)  # Print each answer as it's generated"
      ],
      "metadata": {
        "id": "Om0Hwr2mCT5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "generate_answers(\"questions.txt\", \"model_answers.txt\")"
      ],
      "metadata": {
        "id": "HCVCF2Vo1MBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}